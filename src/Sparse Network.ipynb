{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Management and Boilerplate Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Separation\n",
    "\n",
    "The data is separated into training, validation, and testing pools using a random number generator always seeded with the same value (so separation occurs the same way every time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>discharged_home</th>\n",
       "      <th>time_in_hospital</th>\n",
       "      <th>num_lab_procedures</th>\n",
       "      <th>num_procedures</th>\n",
       "      <th>num_medications</th>\n",
       "      <th>number_outpatient</th>\n",
       "      <th>number_emergency</th>\n",
       "      <th>number_inpatient</th>\n",
       "      <th>number_diagnoses</th>\n",
       "      <th>...</th>\n",
       "      <th>insulin_No</th>\n",
       "      <th>insulin_Steady</th>\n",
       "      <th>insulin_Up</th>\n",
       "      <th>change_Ch</th>\n",
       "      <th>change_No</th>\n",
       "      <th>diabetesMed_No</th>\n",
       "      <th>diabetesMed_Yes</th>\n",
       "      <th>OUTPUT_&lt;30</th>\n",
       "      <th>OUTPUT_&gt;30</th>\n",
       "      <th>OUTPUT_NO</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.15</td>\n",
       "      <td>1</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>0.442748</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.2125</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.25</td>\n",
       "      <td>1</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.076336</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.1500</td>\n",
       "      <td>0.047619</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.047619</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.35</td>\n",
       "      <td>1</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.328244</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.1875</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.45</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.381679</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0875</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.75</td>\n",
       "      <td>1</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>0.549618</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1375</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 70 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    age  discharged_home  time_in_hospital  num_lab_procedures  \\\n",
       "1  0.15                1          0.153846            0.442748   \n",
       "2  0.25                1          0.076923            0.076336   \n",
       "3  0.35                1          0.076923            0.328244   \n",
       "4  0.45                1          0.000000            0.381679   \n",
       "7  0.75                1          0.307692            0.549618   \n",
       "\n",
       "   num_procedures  num_medications  number_outpatient  number_emergency  \\\n",
       "1        0.000000           0.2125           0.000000               0.0   \n",
       "2        0.833333           0.1500           0.047619               0.0   \n",
       "3        0.166667           0.1875           0.000000               0.0   \n",
       "4        0.000000           0.0875           0.000000               0.0   \n",
       "7        0.000000           0.1375           0.000000               0.0   \n",
       "\n",
       "   number_inpatient  number_diagnoses  ...  insulin_No  insulin_Steady  \\\n",
       "1          0.000000          0.533333  ...           0               0   \n",
       "2          0.047619          0.333333  ...           1               0   \n",
       "3          0.000000          0.400000  ...           0               0   \n",
       "4          0.000000          0.266667  ...           0               1   \n",
       "7          0.000000          0.466667  ...           1               0   \n",
       "\n",
       "   insulin_Up  change_Ch  change_No  diabetesMed_No  diabetesMed_Yes  \\\n",
       "1           1          1          0               0                1   \n",
       "2           0          0          1               0                1   \n",
       "3           1          1          0               0                1   \n",
       "4           0          1          0               0                1   \n",
       "7           0          0          1               0                1   \n",
       "\n",
       "   OUTPUT_<30  OUTPUT_>30  OUTPUT_NO  \n",
       "1           0           1          0  \n",
       "2           0           0          1  \n",
       "3           0           0          1  \n",
       "4           0           0          1  \n",
       "7           0           1          0  \n",
       "\n",
       "[5 rows x 70 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"../data/preprocessed.csv\")\n",
    "df.drop([\"Unnamed: 0\"], axis=1, inplace=True)\n",
    "\n",
    "# Pull out validation (15%) and test (15%) data from training (70%) data\n",
    "df_valid_test = df.sample(frac=0.3, random_state=0xda)\n",
    "df_training = df.drop(df_valid_test.index)\n",
    "\n",
    "df_valid = df_valid_test.sample(frac=0.5, random_state=0xdb)\n",
    "df_test = df_valid_test.drop(df_valid.index, axis=0)\n",
    "\n",
    "df_training.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batches and Categorization\n",
    "\n",
    "The following class can be used as an iterable in a `for` loop. It takes a dynamic collection of data separated into pools (e.g. categories like readmission vs. no readmission) and produces a batch which samples equally frequently from each pool. The iterable stops when all data in the largest pool have been iterated over once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_INPUTS = 67\n",
    "\n",
    "class TrainingBatches:\n",
    "\n",
    "    def __init__(self, nn, batch_size=16):\n",
    "        self.nn = nn\n",
    "        self.batch_size = batch_size\n",
    "        self.bounds = max([d.shape[0] for d in nn.training_data])\n",
    "        self.it = 0\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        if self.it >= self.bounds:\n",
    "            raise StopIteration\n",
    "\n",
    "        output = np.concatenate([self.getSlice(d, self.it, self.batch_size) for d in self.nn.training_data])\n",
    "        self.it += self.batch_size\n",
    "        return output[:, :N_INPUTS], output[:, N_INPUTS:]\n",
    "\n",
    "    # Code must work with Python 2 and 3\n",
    "    next = __next__\n",
    "\n",
    "    def getSlice(self, array, start, length):\n",
    "        start = start % array.shape[0]\n",
    "        if start + length >= array.shape[0]:\n",
    "            return np.concatenate((array[start:], array[:(start + length) % array.shape[0]]), axis=0)\n",
    "        else:\n",
    "            return array[start:start + length]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CategoricalPreprocessor:\n",
    "    \n",
    "    def __init__(self, df, output_categories):\n",
    "        self.output_categories = output_categories\n",
    "        self.df = df\n",
    "        self.is_modified = False\n",
    "        \n",
    "    def modifyDatafile(self):\n",
    "        \n",
    "        if self.is_modified:\n",
    "            return self.df\n",
    "        \n",
    "        self.is_modified = True\n",
    "        \n",
    "        if self.output_categories == \"any\":\n",
    "            self.df[\"OUTPUT_ANY\"] = self.df[\"OUTPUT_<30\"] + self.df[\"OUTPUT_>30\"]\n",
    "            self.df.drop([\"OUTPUT_<30\", \"OUTPUT_>30\"], axis=1, inplace=True)\n",
    "\n",
    "        elif self.output_categories == \"rapid\":\n",
    "            self.df[\"OUTPUT_NO\"] = self.df[\"OUTPUT_>30\"] + self.df[\"OUTPUT_NO\"]\n",
    "            self.df.drop([\"OUTPUT_>30\"], axis=1, inplace=True)\n",
    "            \n",
    "        return self.df\n",
    "    \n",
    "    def getArraysByOutput(self):\n",
    "        \n",
    "        if not self.is_modified:\n",
    "            self.modifyDatafile()\n",
    "            \n",
    "        if self.output_categories == \"three\":\n",
    "            return (\n",
    "                self.df[self.df[\"OUTPUT_<30\"] == 1].to_numpy().astype(\"float32\"),\n",
    "                self.df[self.df[\"OUTPUT_>30\"] == 1].to_numpy().astype(\"float32\"),\n",
    "                self.df[self.df[\"OUTPUT_NO\"] == 1].to_numpy().astype(\"float32\")\n",
    "            )\n",
    "\n",
    "        elif self.output_categories == \"any\":\n",
    "            return (\n",
    "                self.df[self.df[\"OUTPUT_ANY\"] == 1].to_numpy().astype(\"float32\"),\n",
    "                self.df[self.df[\"OUTPUT_NO\"] == 1].to_numpy().astype(\"float32\")\n",
    "            )\n",
    "\n",
    "        else:\n",
    "            return (\n",
    "                self.df[self.df[\"OUTPUT_<30\"] == 1].to_numpy().astype(\"float32\"),\n",
    "                self.df[self.df[\"OUTPUT_NO\"] == 1].to_numpy().astype(\"float32\")\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network\n",
    "\n",
    "From study of existing research and experimentation with tuning hyperparameters, the following configuration gives pretty good results for this particular problem:\n",
    "\n",
    " * 70 neurons in 1st hidden layer, 20 neurons in 2nd hidden layer; both layers have sigmoidal activation functions to make transition to a zero-or-one function more natural\n",
    " * Grouping outputs into two categories: 1) the patient is readmitted within 30 days, and 2) the patient is not readmitted within 30 days; this is described in the code as \"rapid\"\n",
    " * Dropout rate of 0.1\n",
    " * Batch size of 128, 64 of each output category to have balanced sampling; this is implemented using the `TrainingBatches` class\n",
    " \n",
    "In order to make the neural network more sparse (i.e. there are many weight parameters that are 0), the algorithm described by [Srivinas, et al.](https://arxiv.org/pdf/1611.06694.pdf) will be used.\n",
    "\n",
    "A layer of weights $W_i$ is masked by a matrix of parameters $g_i^S \\in \\{0, 1\\}^{n_i}$. $g_i^S$ is sampled as an interpretation of the most likely outcome of a bernoulli trial: $g_i > 0.5$. The optimization algorithm chooses evaluative parameters $\\hat \\theta = \\{W, B\\}$ and sparsification parameters $\\hat \\phi = \\{g^S\\}$ by minimizing the following function:\n",
    "\n",
    "$$\\mathrm{loss}(\\hat y(\\theta, \\phi), y) + \\lambda_1 \\sum_{i=1}^m g_i(1 - g_i) + \\lambda_2 \\sum_{i=1}^m g_i$$\n",
    "\n",
    "The sparsification parameters are represented as $g^S = \\mathrm{bernoulli}(g)$ such that $g = \\mathrm{clip}(x)$ and $\\frac{dg^S}{dg} = 1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support for custom gradients\n",
    "\n",
    " * [StackOverflow](https://stackoverflow.com/questions/39921607/how-to-make-a-custom-activation-function-with-only-python-in-tensorflow)\n",
    " * [GitHub Gist](https://gist.github.com/harpone/3453185b41d8d985356cbe5e57d67342)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.custom_gradient\n",
    "def clip(x):\n",
    "    def grad(dy):\n",
    "        return tf.constant(1, shape=x.shape, dtype=\"float32\")\n",
    "    return tf.clip_by_value(x, 0, 1), grad\n",
    "\n",
    "@tf.custom_gradient\n",
    "def bernoulli(x):\n",
    "    def grad(dy):\n",
    "        return tf.constant(1, shape=x.shape, dtype=\"float32\")\n",
    "    return tf.dtypes.cast(tf.greater(x, 0.5), \"float\"), grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sparse Tensors\n",
    "\n",
    "The following function creates a random tensor with a certain sparsity. The parameters `zero` and `one` define the values for sparse and non-sparse elements.\n",
    "\n",
    "Another function calculates the sparsity of a tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPARSITY_THRESHOLD = 0.5\n",
    "\n",
    "def randomSparseMatrix(shape, sparsity, zero, one):\n",
    "    return one - (one - zero) * np.random.binomial(1, sparsity, shape)\n",
    "\n",
    "def sparsity(tensor):\n",
    "    return sum(tf.less(tensor, SPARSITY_THRESHOLD)) / tensor.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "\n",
    "    def __init__(self, df, df_validation, layer_sizes, output_categories, dropout, batch_size, initial_sparsity):\n",
    "        self.dropout = dropout\n",
    "        self.output_categories = output_categories\n",
    "        self.batch_size = batch_size\n",
    "        self.initial_sparsity = initial_sparsity\n",
    "        \n",
    "        # Prepare training data\n",
    "        training_processor = CategoricalPreprocessor(df, output_categories)\n",
    "        self.training_data = training_processor.getArraysByOutput()\n",
    "        \n",
    "        # Prepare training batches\n",
    "        training_in_out = list(iter(TrainingBatches(self, 32)))\n",
    "        self.training_in = np.concatenate([i for i, o in training_in_out])\n",
    "        self.training_out = np.concatenate([o for i, o in training_in_out])\n",
    "        \n",
    "        # Prepare validation data\n",
    "        validation_processor = CategoricalPreprocessor(df_validation, output_categories)\n",
    "        self.validation_data = validation_processor.modifyDatafile().to_numpy()\n",
    "        self.validation_in = self.validation_data[:, :N_INPUTS]\n",
    "        self.validation_out = self.validation_data[:, N_INPUTS:]\n",
    "        \n",
    "        # Structure\n",
    "        self.layer_sizes = (N_INPUTS,) + layer_sizes + (self.validation_out.shape[1],)\n",
    "        \n",
    "        # Variables\n",
    "        self.X = tf.placeholder(\"float32\", [None, self.layer_sizes[0]])\n",
    "        self.Y = tf.placeholder(\"float32\", [None, self.layer_sizes[-1]])\n",
    "        self.W = [tf.Variable(\n",
    "                tf.random_normal([self.layer_sizes[i], self.layer_sizes[i+1]]),\n",
    "            dtype=\"float32\") for i in range(len(self.layer_sizes) - 1)]\n",
    "        self.B = [tf.Variable(\n",
    "                tf.random_normal([sz]),\n",
    "            dtype=\"float32\") for sz in self.layer_sizes[1:]]\n",
    "        self.Gx = [tf.Variable(\n",
    "                randomSparseMatrix([self.layer_sizes[i], self.layer_sizes[i+1]], self.initial_sparsity, 0.49, 1.0),\n",
    "            dtype=\"float32\") for i in range(len(self.layer_sizes) - 1)]\n",
    "        self.a = tf.constant(1.0, dtype=\"float32\")\n",
    "        self.L1 = tf.constant(0.01, dtype=\"float32\")\n",
    "        self.L2 = tf.constant(0.01, dtype=\"float32\")\n",
    "        \n",
    "        # Make the classifier graph\n",
    "        self.classifier = self.getClassifier()\n",
    "        \n",
    "        # Optimization\n",
    "        self.loss = self.getLoss()\n",
    "        self.trainer = tf.train.GradientDescentOptimizer(learning_rate=0.05).minimize(self.loss)    \n",
    "\n",
    "        \n",
    "    def getLoss(self):\n",
    "        loss = tf.reduce_mean(tf.losses.softmax_cross_entropy(self.Y, self.classifier))\n",
    "        G = [clip(Gx) for Gx in self.Gx]\n",
    "        G_bimodal = self.L1 * tf.math.reduce_sum([tf.math.reduce_sum(tf.multiply(g, 1 - g)) for g in G])\n",
    "        G_magnitude = self.L2 * tf.math.reduce_sum([tf.math.reduce_sum(g) for g in G])\n",
    "        return loss\n",
    "        \n",
    "\n",
    "    def getClassifier(self):\n",
    "        \n",
    "        def layer(n):\n",
    "            \n",
    "            # Input, with base case\n",
    "            X = self.X if n == 0 else layer(n - 1)\n",
    "            X_dropped = tf.nn.dropout(X, rate=self.dropout)\n",
    "            \n",
    "            # Sample from G\n",
    "            #G = clip(self.Gx[n])\n",
    "            #GS = bernoulli(G)\n",
    "            \n",
    "            # Activation function\n",
    "            f = tf.nn.sigmoid\n",
    "            print(\"Layer {}, f: {}\".format(n, f))\n",
    "            #return f(self.a * (tf.matmul(X_dropped, tf.multiply(self.W[n], GS)) + self.B[n]))\n",
    "            return f(self.a * (tf.matmul(X_dropped, self.W[n]) + self.B[n]))\n",
    "        \n",
    "        return tf.nn.softmax(layer(len(self.layer_sizes) - 2))\n",
    "    \n",
    "    \n",
    "    def train(self, steps, epochs):\n",
    "        \n",
    "        with tf.Session(config=tf.ConfigProto(log_device_placement=True)) as sess:\n",
    "            init = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())\n",
    "            sess.run(init)\n",
    "            \n",
    "            for step in range(steps):\n",
    "                \n",
    "                for epoch in range(epochs):\n",
    "                    totalLoss = 0.0\n",
    "                    numBatches = 0\n",
    "                    \n",
    "                    for batch_in, batch_out in iter(TrainingBatches(self, self.batch_size)):\n",
    "                        _, loss = sess.run([self.trainer, self.loss], feed_dict={self.X: batch_in, self.Y:batch_out})\n",
    "                        totalLoss += loss\n",
    "                        numBatches += 1\n",
    "                        \n",
    "                    loss = totalLoss / numBatches / self.batch_size\n",
    "                    accuracy = self.validate(sess)\n",
    "                    \n",
    "                    print(\"Step {}, Epoch {}: Loss = {}, Accuracy = {}\".format(step, epoch, loss, accuracy))\n",
    "                        \n",
    "                        \n",
    "    def validate(self, sess=None):\n",
    "        \n",
    "        if sess == None:\n",
    "            with tf.Session(config=tf.ConfigProto(log_device_placement=True)) as sess:\n",
    "                output = self.validate(sess)\n",
    "            return output\n",
    "        \n",
    "        pred = self.classifier\n",
    "        percentageCorrect = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(pred, 1), tf.argmax(self.Y, 1)), \"float32\"))\n",
    "        accuracy = sess.run([percentageCorrect], feed_dict={self.X: self.validation_in, self.Y: self.validation_out})\n",
    "        return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def newNeuralNetwork():\n",
    "    return NeuralNetwork(**{\n",
    "        \"df\": df_training.copy(),\n",
    "        \"df_validation\": df_valid.copy(),\n",
    "        \"layer_sizes\": (70, 20),\n",
    "        \"output_categories\": \"rapid\",\n",
    "        \"dropout\": 0.1,\n",
    "        \"batch_size\": 64,\n",
    "        \"initial_sparsity\": 0.5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1024 04:16:18.400652 140455268685632 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/losses/losses_impl.py:121: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 0, f: <function sigmoid at 0x7fbdb379a410>\n",
      "Layer 1, f: <function sigmoid at 0x7fbdb379a410>\n",
      "Layer 2, f: <function sigmoid at 0x7fbdb379a410>\n"
     ]
    }
   ],
   "source": [
    "nn = newNeuralNetwork()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0, Epoch 0: Loss = 0.0108771054645, Accuracy = [0.5634521]\n",
      "Step 0, Epoch 1: Loss = 0.0108465238785, Accuracy = [0.50989866]\n",
      "Step 0, Epoch 2: Loss = 0.010845517405, Accuracy = [0.5265418]\n",
      "Step 0, Epoch 3: Loss = 0.0108368456072, Accuracy = [0.53023285]\n",
      "Step 0, Epoch 4: Loss = 0.0108349993554, Accuracy = [0.5116435]\n",
      "Step 0, Epoch 5: Loss = 0.0108261373153, Accuracy = [0.5318435]\n",
      "Step 0, Epoch 6: Loss = 0.0108272800501, Accuracy = [0.5174149]\n",
      "Step 0, Epoch 7: Loss = 0.010818961235, Accuracy = [0.51600564]\n",
      "Step 0, Epoch 8: Loss = 0.0108219454904, Accuracy = [0.5231193]\n",
      "Step 0, Epoch 9: Loss = 0.0108225007134, Accuracy = [0.52298504]\n",
      "Step 0, Epoch 10: Loss = 0.0108160266705, Accuracy = [0.52036774]\n",
      "Step 0, Epoch 11: Loss = 0.0108078484591, Accuracy = [0.5257365]\n",
      "Step 0, Epoch 12: Loss = 0.0108095824386, Accuracy = [0.53499764]\n",
      "Step 0, Epoch 13: Loss = 0.0108055456504, Accuracy = [0.53915846]\n",
      "Step 0, Epoch 14: Loss = 0.010804216095, Accuracy = [0.5303671]\n",
      "Step 0, Epoch 15: Loss = 0.0108015545018, Accuracy = [0.5233877]\n",
      "Step 0, Epoch 16: Loss = 0.0108034830871, Accuracy = [0.5242601]\n",
      "Step 0, Epoch 17: Loss = 0.0107999736886, Accuracy = [0.51788473]\n",
      "Step 0, Epoch 18: Loss = 0.0107932012346, Accuracy = [0.5249983]\n",
      "Step 0, Epoch 19: Loss = 0.0107973028324, Accuracy = [0.5303671]\n",
      "Step 1, Epoch 0: Loss = 0.0107919437528, Accuracy = [0.5206362]\n",
      "Step 1, Epoch 1: Loss = 0.0107978773782, Accuracy = [0.53795046]\n",
      "Step 1, Epoch 2: Loss = 0.0107905454353, Accuracy = [0.5308369]\n",
      "Step 1, Epoch 3: Loss = 0.0107882980137, Accuracy = [0.5325146]\n",
      "Step 1, Epoch 4: Loss = 0.0107906551834, Accuracy = [0.52815247]\n",
      "Step 1, Epoch 5: Loss = 0.0107850977331, Accuracy = [0.53352123]\n",
      "Step 1, Epoch 6: Loss = 0.010788634109, Accuracy = [0.5374136]\n",
      "Step 1, Epoch 7: Loss = 0.0107880863492, Accuracy = [0.53063554]\n",
      "Step 1, Epoch 8: Loss = 0.0107826987951, Accuracy = [0.54311794]\n",
      "Step 1, Epoch 9: Loss = 0.0107822690579, Accuracy = [0.5356016]\n",
      "Step 1, Epoch 10: Loss = 0.0107796867073, Accuracy = [0.53902423]\n",
      "Step 1, Epoch 11: Loss = 0.0107796259286, Accuracy = [0.5451312]\n",
      "Step 1, Epoch 12: Loss = 0.0107711701473, Accuracy = [0.53144085]\n",
      "Step 1, Epoch 13: Loss = 0.0107718063557, Accuracy = [0.5373465]\n",
      "Step 1, Epoch 14: Loss = 0.0107732451862, Accuracy = [0.5490907]\n",
      "Step 1, Epoch 15: Loss = 0.0107678097875, Accuracy = [0.5340581]\n",
      "Step 1, Epoch 16: Loss = 0.0107705433924, Accuracy = [0.5274814]\n",
      "Step 1, Epoch 17: Loss = 0.0107721104456, Accuracy = [0.5359372]\n",
      "Step 1, Epoch 18: Loss = 0.0107705485799, Accuracy = [0.53788334]\n",
      "Step 1, Epoch 19: Loss = 0.010765398661, Accuracy = [0.5414402]\n"
     ]
    }
   ],
   "source": [
    "nn.train(2, 20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
