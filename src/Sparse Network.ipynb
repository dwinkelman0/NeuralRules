{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Management and Boilerplate Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Separation\n",
    "\n",
    "The data is separated into training, validation, and testing pools using a random number generator always seeded with the same value (so separation occurs the same way every time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>discharged_home</th>\n",
       "      <th>time_in_hospital</th>\n",
       "      <th>num_lab_procedures</th>\n",
       "      <th>num_procedures</th>\n",
       "      <th>num_medications</th>\n",
       "      <th>number_outpatient</th>\n",
       "      <th>number_emergency</th>\n",
       "      <th>number_inpatient</th>\n",
       "      <th>number_diagnoses</th>\n",
       "      <th>...</th>\n",
       "      <th>insulin_No</th>\n",
       "      <th>insulin_Steady</th>\n",
       "      <th>insulin_Up</th>\n",
       "      <th>change_Ch</th>\n",
       "      <th>change_No</th>\n",
       "      <th>diabetesMed_No</th>\n",
       "      <th>diabetesMed_Yes</th>\n",
       "      <th>OUTPUT_&lt;30</th>\n",
       "      <th>OUTPUT_&gt;30</th>\n",
       "      <th>OUTPUT_NO</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.15</td>\n",
       "      <td>1</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>0.442748</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.2125</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.25</td>\n",
       "      <td>1</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.076336</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.1500</td>\n",
       "      <td>0.047619</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.047619</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.35</td>\n",
       "      <td>1</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.328244</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.1875</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.45</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.381679</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0875</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.75</td>\n",
       "      <td>1</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>0.549618</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1375</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 70 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    age  discharged_home  time_in_hospital  num_lab_procedures  \\\n",
       "1  0.15                1          0.153846            0.442748   \n",
       "2  0.25                1          0.076923            0.076336   \n",
       "3  0.35                1          0.076923            0.328244   \n",
       "4  0.45                1          0.000000            0.381679   \n",
       "7  0.75                1          0.307692            0.549618   \n",
       "\n",
       "   num_procedures  num_medications  number_outpatient  number_emergency  \\\n",
       "1        0.000000           0.2125           0.000000               0.0   \n",
       "2        0.833333           0.1500           0.047619               0.0   \n",
       "3        0.166667           0.1875           0.000000               0.0   \n",
       "4        0.000000           0.0875           0.000000               0.0   \n",
       "7        0.000000           0.1375           0.000000               0.0   \n",
       "\n",
       "   number_inpatient  number_diagnoses  ...  insulin_No  insulin_Steady  \\\n",
       "1          0.000000          0.533333  ...           0               0   \n",
       "2          0.047619          0.333333  ...           1               0   \n",
       "3          0.000000          0.400000  ...           0               0   \n",
       "4          0.000000          0.266667  ...           0               1   \n",
       "7          0.000000          0.466667  ...           1               0   \n",
       "\n",
       "   insulin_Up  change_Ch  change_No  diabetesMed_No  diabetesMed_Yes  \\\n",
       "1           1          1          0               0                1   \n",
       "2           0          0          1               0                1   \n",
       "3           1          1          0               0                1   \n",
       "4           0          1          0               0                1   \n",
       "7           0          0          1               0                1   \n",
       "\n",
       "   OUTPUT_<30  OUTPUT_>30  OUTPUT_NO  \n",
       "1           0           1          0  \n",
       "2           0           0          1  \n",
       "3           0           0          1  \n",
       "4           0           0          1  \n",
       "7           0           1          0  \n",
       "\n",
       "[5 rows x 70 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"../data/preprocessed.csv\")\n",
    "df.drop([\"Unnamed: 0\"], axis=1, inplace=True)\n",
    "\n",
    "# Pull out validation (15%) and test (15%) data from training (70%) data\n",
    "df_valid_test = df.sample(frac=0.3, random_state=0xda)\n",
    "df_training = df.drop(df_valid_test.index)\n",
    "\n",
    "df_valid = df_valid_test.sample(frac=0.5, random_state=0xdb)\n",
    "df_test = df_valid_test.drop(df_valid.index, axis=0)\n",
    "\n",
    "df_training.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batches and Categorization\n",
    "\n",
    "The following class can be used as an iterable in a `for` loop. It takes a dynamic collection of data separated into pools (e.g. categories like readmission vs. no readmission) and produces a batch which samples equally frequently from each pool. The iterable stops when all data in the largest pool have been iterated over once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_INPUTS = 67\n",
    "\n",
    "class TrainingBatches:\n",
    "\n",
    "    def __init__(self, nn, batch_size=16):\n",
    "        self.nn = nn\n",
    "        self.batch_size = batch_size\n",
    "        self.bounds = max([d.shape[0] for d in nn.training_data])\n",
    "        self.it = 0\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        if self.it >= self.bounds:\n",
    "            raise StopIteration\n",
    "\n",
    "        output = np.concatenate([self.getSlice(d, self.it, self.batch_size) for d in self.nn.training_data])\n",
    "        self.it += self.batch_size\n",
    "        return output[:, :N_INPUTS], output[:, N_INPUTS:]\n",
    "\n",
    "    # Code must work with Python 2 and 3\n",
    "    next = __next__\n",
    "\n",
    "    def getSlice(self, array, start, length):\n",
    "        start = start % array.shape[0]\n",
    "        if start + length >= array.shape[0]:\n",
    "            return np.concatenate((array[start:], array[:(start + length) % array.shape[0]]), axis=0)\n",
    "        else:\n",
    "            return array[start:start + length]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CategoricalPreprocessor:\n",
    "    \n",
    "    def __init__(self, df, output_categories):\n",
    "        self.output_categories = output_categories\n",
    "        self.df = df\n",
    "        self.is_modified = False\n",
    "        \n",
    "    def modifyDatafile(self):\n",
    "        \n",
    "        if self.is_modified:\n",
    "            return self.df\n",
    "        \n",
    "        self.is_modified = True\n",
    "        \n",
    "        if self.output_categories == \"any\":\n",
    "            self.df[\"OUTPUT_ANY\"] = self.df[\"OUTPUT_<30\"] + self.df[\"OUTPUT_>30\"]\n",
    "            self.df.drop([\"OUTPUT_<30\", \"OUTPUT_>30\"], axis=1, inplace=True)\n",
    "\n",
    "        elif self.output_categories == \"rapid\":\n",
    "            self.df[\"OUTPUT_NO\"] = self.df[\"OUTPUT_>30\"] + self.df[\"OUTPUT_NO\"]\n",
    "            self.df.drop([\"OUTPUT_>30\"], axis=1, inplace=True)\n",
    "            \n",
    "        return self.df\n",
    "    \n",
    "    def getArraysByOutput(self):\n",
    "        \n",
    "        if not self.is_modified:\n",
    "            self.modifyDatafile()\n",
    "            \n",
    "        if self.output_categories == \"three\":\n",
    "            return (\n",
    "                self.df[self.df[\"OUTPUT_<30\"] == 1].to_numpy().astype(\"float32\"),\n",
    "                self.df[self.df[\"OUTPUT_>30\"] == 1].to_numpy().astype(\"float32\"),\n",
    "                self.df[self.df[\"OUTPUT_NO\"] == 1].to_numpy().astype(\"float32\")\n",
    "            )\n",
    "\n",
    "        elif self.output_categories == \"any\":\n",
    "            return (\n",
    "                self.df[self.df[\"OUTPUT_ANY\"] == 1].to_numpy().astype(\"float32\"),\n",
    "                self.df[self.df[\"OUTPUT_NO\"] == 1].to_numpy().astype(\"float32\")\n",
    "            )\n",
    "\n",
    "        else:\n",
    "            return (\n",
    "                self.df[self.df[\"OUTPUT_<30\"] == 1].to_numpy().astype(\"float32\"),\n",
    "                self.df[self.df[\"OUTPUT_NO\"] == 1].to_numpy().astype(\"float32\")\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network\n",
    "\n",
    "From study of existing research and experimentation with tuning hyperparameters, the following configuration gives pretty good results for this particular problem:\n",
    "\n",
    " * 70 neurons in 1st hidden layer, 20 neurons in 2nd hidden layer; both layers have sigmoidal activation functions to make transition to a zero-or-one function more natural\n",
    " * Grouping outputs into two categories: 1) the patient is readmitted within 30 days, and 2) the patient is not readmitted within 30 days; this is described in the code as \"rapid\"\n",
    " * Dropout rate of 0.1\n",
    " * Batch size of 128, 64 of each output category to have balanced sampling; this is implemented using the `TrainingBatches` class\n",
    " \n",
    "In order to make the neural network more sparse (i.e. there are many weight parameters that are 0), the algorithm described by [Srivinas, et al.](https://arxiv.org/pdf/1611.06694.pdf) will be used.\n",
    "\n",
    "A layer of weights $W_i$ is masked by a matrix of parameters $g_i^S \\in \\{0, 1\\}^{n_i}$. $g_i^S$ is sampled as an interpretation of the most likely outcome of a bernoulli trial: $g_i > 0.5$. The optimization algorithm chooses evaluative parameters $\\hat \\theta = \\{W, B\\}$ and sparsification parameters $\\hat \\phi = \\{g^S\\}$ by minimizing the following function:\n",
    "\n",
    "$$\\mathrm{loss}(\\hat y(\\theta, \\phi), y) + \\lambda_1 \\sum_{i=1}^m g_i(1 - g_i) + \\lambda_2 \\sum_{i=1}^m g_i$$\n",
    "\n",
    "The sparsification parameters are represented as $g^S = \\mathrm{bernoulli}(g)$ such that $g = \\mathrm{clip}(x)$ and $\\frac{dg^S}{dg} = 1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support for custom gradients\n",
    "\n",
    " * [StackOverflow](https://stackoverflow.com/questions/39921607/how-to-make-a-custom-activation-function-with-only-python-in-tensorflow)\n",
    " * [GitHub Gist](https://gist.github.com/harpone/3453185b41d8d985356cbe5e57d67342)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.custom_gradient\n",
    "def clip(x):\n",
    "    def grad(dy):\n",
    "        return 1\n",
    "    return tf.clip_by_value(x, 0, 1), grad\n",
    "\n",
    "@tf.custom_gradient\n",
    "def bernoulli(x):\n",
    "    def grad(dy):\n",
    "        return 1\n",
    "    return tf.dtypes.cast(tf.greater(x, 0.5), \"float\"), grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sparse Tensors\n",
    "\n",
    "The following function creates a random tensor with a certain sparsity. The parameters `zero` and `one` define the values for sparse and non-sparse elements.\n",
    "\n",
    "Another function calculates the sparsity of a tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPARSITY_THRESHOLD = 0.5\n",
    "\n",
    "def randomSparseMatrix(shape, sparsity, zero, one):\n",
    "    return one - (one - zero) * np.random.binomial(1, sparsity, shape)\n",
    "\n",
    "def sparsity(tensor):\n",
    "    return sum(tf.less(tensor, SPARSITY_THRESHOLD)) / tensor.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "\n",
    "    def __init__(self, df, df_validation, layer_sizes, output_categories, dropout, batch_size, initial_sparsity):\n",
    "        self.dropout = dropout\n",
    "        self.output_categories = output_categories\n",
    "        self.batch_size = batch_size\n",
    "        self.initial_sparsity = initial_sparsity\n",
    "        \n",
    "        # Prepare training data\n",
    "        training_processor = CategoricalPreprocessor(df, output_categories)\n",
    "        self.training_data = training_processor.getArraysByOutput()\n",
    "        \n",
    "        # Prepare training batches\n",
    "        training_in_out = list(iter(TrainingBatches(self, 32)))\n",
    "        self.training_in = np.concatenate([i for i, o in training_in_out])\n",
    "        self.training_out = np.concatenate([o for i, o in training_in_out])\n",
    "        \n",
    "        # Prepare validation data\n",
    "        validation_processor = CategoricalPreprocessor(df_validation, output_categories)\n",
    "        self.validation_data = validation_processor.modifyDatafile().to_numpy()\n",
    "        self.validation_in = self.validation_data[:, :N_INPUTS]\n",
    "        self.validation_out = self.validation_data[:, N_INPUTS:]\n",
    "        \n",
    "        # Structure\n",
    "        self.layer_sizes = (N_INPUTS,) + layer_sizes + (self.validation_out.shape[1],)\n",
    "        \n",
    "        # Variables\n",
    "        self.X = tf.placeholder(\"float32\", [None, self.layer_sizes[0]])\n",
    "        self.Y = tf.placeholder(\"float32\", [None, self.layer_sizes[-1]])\n",
    "        self.W = [tf.Variable(\n",
    "                tf.random_normal([self.layer_sizes[i], self.layer_sizes[i+1]]),\n",
    "            dtype=\"float32\") for i in range(len(self.layer_sizes) - 1)]\n",
    "        self.B = [tf.Variable(\n",
    "                tf.random_normal([sz]),\n",
    "            dtype=\"float32\") for sz in self.layer_sizes[1:]]\n",
    "        self.Gx = [tf.Variable(\n",
    "                randomSparseMatrix([self.layer_sizes[i], self.layer_sizes[i+1]], self.initial_sparsity, 0.49, 1.0),\n",
    "            dtype=\"float32\") for i in range(len(self.layer_sizes) - 1)]\n",
    "        self.a = tf.constant(1.0, dtype=\"float32\")\n",
    "        self.L1 = tf.constant(0.01, dtype=\"float32\")\n",
    "        self.L2 = tf.constant(0.01, dtype=\"float32\")\n",
    "        \n",
    "        # Make the classifier graph\n",
    "        self.classifier = self.getClassifier()\n",
    "        \n",
    "        # Optimization\n",
    "        self.loss = self.getLoss()\n",
    "        self.trainer = tf.train.AdamOptimizer(learning_rate=0.01).minimize(self.loss)    \n",
    "\n",
    "        \n",
    "    def getLoss(self):\n",
    "        loss = tf.reduce_mean(tf.losses.softmax_cross_entropy(self.Y, self.classifier))\n",
    "        G = [clip(Gx) for Gx in self.Gx]\n",
    "        G_bimodal = self.L1 * tf.math.reduce_sum([tf.math.reduce_sum(tf.multiply(g, 1 - g)) for g in G])\n",
    "        G_magnitude = self.L2 * tf.math.reduce_sum([tf.math.reduce_sum(g) for g in G])\n",
    "        return loss + G_bimodal + G_magnitude\n",
    "        \n",
    "\n",
    "    def getClassifier(self):\n",
    "        \n",
    "        def layer(n):\n",
    "            \n",
    "            # Input, with base case\n",
    "            X = self.X if n == 0 else layer(n - 1)\n",
    "            X_dropped = tf.nn.dropout(X, 1 - self.dropout)\n",
    "            \n",
    "            # Sample from G\n",
    "            G = clip(self.Gx[n])\n",
    "            GS = bernoulli(G)\n",
    "            \n",
    "            # Activation function\n",
    "            f = tf.nn.softmax if len(self.layer_sizes) else tf.nn.sigmoid\n",
    "            return f(self.a * (tf.matmul(X_dropped, tf.multiply(self.W[n], GS)) + self.B[n]))\n",
    "        \n",
    "        return layer(len(self.layer_sizes) - 2)\n",
    "    \n",
    "    \n",
    "    def train(self, steps, epochs):\n",
    "        \n",
    "        with tf.Session(config=tf.ConfigProto(log_device_placement=True)) as sess:\n",
    "            init = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())\n",
    "            sess.run(init)\n",
    "            \n",
    "            for step in range(steps):\n",
    "                \n",
    "                for epoch in range(epochs):\n",
    "                    totalLoss = 0.0\n",
    "                    numBatches = 0\n",
    "                    \n",
    "                    for batch_in, batch_out in iter(TrainingBatches(self, self.batch_size)):\n",
    "                        _, loss = sess.run([self.trainer, self.loss], feed_dict={self.X: batch_in, self.Y:batch_out})\n",
    "                        totalLoss += loss\n",
    "                        numBatches += 1\n",
    "                        \n",
    "                    loss = totalLoss / numBatches / self.batch_size\n",
    "                    accuracy = self.validate(sess)\n",
    "                    \n",
    "                    print(\"Step {}, Epoch {}: Loss = {}, Accuracy = {}\".format(step, epoch, loss, accuracy))\n",
    "                        \n",
    "                        \n",
    "    def validate(self, sess=None):\n",
    "        \n",
    "        if sess == None:\n",
    "            with tf.Session(config=tf.ConfigProto(log_device_placement=True)) as sess:\n",
    "                output = self.validate(sess)\n",
    "            return output\n",
    "        \n",
    "        pred = tf.nn.softmax(self.classifier)\n",
    "        percentageCorrect = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(pred, 1), tf.argmax(self.Y, 1))))\n",
    "        accuracy = sess.run([percentageCorrect], feed_dict={self.X: self.validation_in, self.Y: self.validation_out})\n",
    "        return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def newNeuralNetwork():\n",
    "    return NeuralNetwork(**{\n",
    "        \"df\": df_training.copy(),\n",
    "        \"df_validation\": df_valid.copy(),\n",
    "        \"layer_sizes\": (70, 20),\n",
    "        \"output_categories\": \"rapid\",\n",
    "        \"dropout\": 0.1,\n",
    "        \"batch_size\": 64,\n",
    "        \"initial_sparsity\": 0.5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'int' object has no attribute 'name'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-0311489ff72b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnewNeuralNetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-13-b5ffe17b7d43>\u001b[0m in \u001b[0;36mnewNeuralNetwork\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;34m\"dropout\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;34m\"batch_size\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \"initial_sparsity\": 0.5})\n\u001b[0m",
      "\u001b[0;32m<ipython-input-12-8276161a3d66>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, df, df_validation, layer_sizes, output_categories, dropout, batch_size, initial_sparsity)\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;31m# Optimization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdamOptimizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mminimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/optimizer.pyc\u001b[0m in \u001b[0;36mminimize\u001b[0;34m(self, loss, global_step, var_list, gate_gradients, aggregation_method, colocate_gradients_with_ops, name, grad_loss)\u001b[0m\n\u001b[1;32m    401\u001b[0m         \u001b[0maggregation_method\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maggregation_method\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m         \u001b[0mcolocate_gradients_with_ops\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolocate_gradients_with_ops\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 403\u001b[0;31m         grad_loss=grad_loss)\n\u001b[0m\u001b[1;32m    404\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    405\u001b[0m     \u001b[0mvars_with_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mv\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgrads_and_vars\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mg\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/optimizer.pyc\u001b[0m in \u001b[0;36mcompute_gradients\u001b[0;34m(self, loss, var_list, gate_gradients, aggregation_method, colocate_gradients_with_ops, grad_loss)\u001b[0m\n\u001b[1;32m    510\u001b[0m         \u001b[0mgate_gradients\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgate_gradients\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mOptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGATE_OP\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    511\u001b[0m         \u001b[0maggregation_method\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maggregation_method\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 512\u001b[0;31m         colocate_gradients_with_ops=colocate_gradients_with_ops)\n\u001b[0m\u001b[1;32m    513\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mgate_gradients\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mOptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGATE_GRAPH\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    514\u001b[0m       \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontrol_flow_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gradients_impl.pyc\u001b[0m in \u001b[0;36mgradients\u001b[0;34m(ys, xs, grad_ys, name, colocate_gradients_with_ops, gate_gradients, aggregation_method, stop_gradients, unconnected_gradients)\u001b[0m\n\u001b[1;32m    156\u001b[0m         \u001b[0mys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_ys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolocate_gradients_with_ops\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[0mgate_gradients\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maggregation_method\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop_gradients\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m         unconnected_gradients)\n\u001b[0m\u001b[1;32m    159\u001b[0m   \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gradients_util.pyc\u001b[0m in \u001b[0;36m_GradientsHelper\u001b[0;34m(ys, xs, grad_ys, name, colocate_gradients_with_ops, gate_gradients, aggregation_method, stop_gradients, unconnected_gradients, src_graph)\u001b[0m\n\u001b[1;32m    745\u001b[0m                       ignore_existing=True):\n\u001b[1;32m    746\u001b[0m                     \u001b[0min_grads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontrol_flow_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_grads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 747\u001b[0;31m           \u001b[0m_LogOpGradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_grads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_grads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    748\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    749\u001b[0m           \u001b[0;31m# If no grad_fn is defined or none of out_grads is available,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gradients_util.pyc\u001b[0m in \u001b[0;36m_LogOpGradients\u001b[0;34m(op, out_grads, in_grads)\u001b[0m\n\u001b[1;32m    906\u001b[0m                \", \".join([x.name for x in out_grads if _FilterGrad(x)]))\n\u001b[1;32m    907\u001b[0m   logging.vlog(1, \"  out --> %s\",\n\u001b[0;32m--> 908\u001b[0;31m                \", \".join([x.name for x in in_grads if _FilterGrad(x)]))\n\u001b[0m\u001b[1;32m    909\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    910\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'int' object has no attribute 'name'"
     ]
    }
   ],
   "source": [
    "nn = newNeuralNetwork()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.training_data[0].dtype"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
